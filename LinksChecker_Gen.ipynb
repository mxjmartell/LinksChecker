{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links Checker v4.1\n",
    "**This Python program uses web scraping to create a site map and find every link on every page to check the response of the URLs and make sure they are active.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Run the following cells in order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% import libraries\n",
    "import requests\n",
    "import certifi\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlsplit\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Enter the starting URL here. Add any links to skip from including in the site map, in quotes separated by a comma. This could include links to Facebook or LinkedIn, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting URL goes here\n",
    "base_url = \"https://www.example.com\" \n",
    "\n",
    "# enter any links to ignore here\n",
    "skip_links = [\"https://www.facebook.com\", \"https://www.linkedin.com\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Run the cell below to create functions and session settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% set web agent variables and headers\n",
    "\n",
    "# add browsers headers\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": f\"{base_url}\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)\n",
    "\n",
    "#%% create site map from base url\n",
    "\n",
    "def site_mapper(base_url, mapped=None):\n",
    "\n",
    "    if mapped is None:\n",
    "        mapped = set()\n",
    "\n",
    "    if not base_url.endswith(\"/\"): # add / to all links for consistency\n",
    "        base_url += \"/\"\n",
    "\n",
    "    response = requests.get(base_url, verify=certifi.where())\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    urls_dict = {}\n",
    "\n",
    "    links = set()\n",
    "\n",
    "    # parse html and find all links\n",
    "    for anchor in soup.find_all(\"a\"):\n",
    "        href = anchor.attrs.get(\"href\")\n",
    "        \n",
    "        if href:\n",
    "            link = urljoin(base_url, href)\n",
    "\n",
    "            fragment = urlsplit(link).fragment # skip any fragments on page\n",
    "            if fragment:\n",
    "                continue\n",
    "            \n",
    "            # only include links that branch from the base url, skip if already in set\n",
    "            if link.startswith(base_url) and link not in mapped:\n",
    "                if link != base_url:\n",
    "                    links.add(link)\n",
    "\n",
    "    mapped.add(base_url)\n",
    "    urls_dict[base_url] = links\n",
    "\n",
    "    # recursive call to iterate through site map\n",
    "    for link in links:\n",
    "        if link not in mapped:\n",
    "            sub_links = site_mapper(link, mapped)\n",
    "            urls_dict.update(sub_links)\n",
    "\n",
    "    return urls_dict\n",
    "\n",
    "\n",
    "#%% read HTML and extract all urls from page\n",
    "\n",
    "def get_links(base_urls, skip_links=skip_links):\n",
    "\n",
    "    urls_dict = {}\n",
    "\n",
    "    for base_url in base_urls:\n",
    "\n",
    "        response = requests.get(base_url, verify=certifi.where())\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        links = set()\n",
    "\n",
    "        # parse html and find all links\n",
    "        for anchor in soup.find_all(\"a\"):\n",
    "            href = anchor.attrs.get(\"href\")\n",
    "            \n",
    "            if href:\n",
    "                link = urljoin(base_url, href)\n",
    "                \n",
    "                fragment = urlsplit(link).fragment # skip any fragments on page\n",
    "                if fragment:\n",
    "                    continue\n",
    "                \n",
    "                if not link.startswith(\"mailto\"): # skip links to email adresses\n",
    "                    if link not in skip_links:\n",
    "                        if link != base_url:\n",
    "                            links.add(link)\n",
    "        \n",
    "        urls_dict[f\"{base_url}\"] = links\n",
    "\n",
    "    \n",
    "    return urls_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Run the cell below to create a site map and collect all links. Warning: will take several minutes to compelte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create site map\n",
    "sitemap = site_mapper(base_url) \n",
    "\n",
    "# get all links from URLs in site map\n",
    "site_links = get_links(sitemap.keys()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Run this cell to check the status of each link. **Warning: this can take up to several hours.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4302e9d3a9b429cb3cf63e978b85819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='success', description='Loading: ', max=1, style=ProgressStyle(bar_color='blue'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% check every url and store link status in a table\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# show progress bar\n",
    "progress = 0\n",
    "progress_bar = widgets.IntProgress(\n",
    "    value = progress,\n",
    "    min = 0,\n",
    "    max = len(site_links.keys()),\n",
    "    description = \"Loading: \",\n",
    "    bar_style = \"success\",\n",
    "    style = {\"bar_color\": \"blue\"},\n",
    "    orientation = \"horizontal\"\n",
    ")\n",
    "display(progress_bar)\n",
    "\n",
    "dict = {\"Base URL\": [], \"Link URL\": [], \"Link Status\": [], \"Status Code\": [], \"Response\": []}\n",
    "\n",
    "for base_url, link_set in site_links.items():\n",
    "    progress += 1\n",
    "    progress_bar.value = progress\n",
    "\n",
    "    for link in link_set:\n",
    "    \n",
    "        dict[\"Base URL\"].append(base_url)\n",
    "        dict[\"Link URL\"].append(link)\n",
    "    \n",
    "        # try-except-pass\n",
    "        retry = 1\n",
    "        while retry < 5:\n",
    "            try:\n",
    "                response = requests.head(link, allow_redirects=True, headers=headers, verify=certifi.where())\n",
    "                retry = 5\n",
    "            except requests.exceptions.SSLError as e:\n",
    "                if retry == 4:\n",
    "                    response = \"SSLError\"\n",
    "                    retry += 1\n",
    "                else:\n",
    "                    time.sleep(1 * (2 * retry))\n",
    "                    retry += 1\n",
    "            except:\n",
    "                response = None\n",
    "\n",
    "        # set response from link\n",
    "        if response == \"SSLError\":\n",
    "            dict[\"Link Status\"].append(\"SSL Error\")\n",
    "            dict[\"Status Code\"].append(\"SSL Error\")\n",
    "            dict[\"Response\"].append(\"SSL Error\")\n",
    "        elif response == None:\n",
    "            dict[\"Link Status\"].append(\"Error\")\n",
    "            dict[\"Status Code\"].append(\"Error\")\n",
    "            dict[\"Response\"].append(\"Error\")\n",
    "        else:\n",
    "            status_code = response.status_code\n",
    "            \n",
    "            if status_code == 200:\n",
    "                dict[\"Link Status\"].append(\"Active\")\n",
    "            else:\n",
    "                dict[\"Link Status\"].append(\"Inactive\")\n",
    "            \n",
    "            dict[\"Status Code\"].append(status_code)\n",
    "            \n",
    "            # define response code\n",
    "            match status_code:\n",
    "                case 200:\n",
    "                    dict[\"Response\"].append(\"Success\")\n",
    "                case 400:\n",
    "                    dict[\"Response\"].append(\"Bad Request\")\n",
    "                case 401:\n",
    "                    dict[\"Response\"].append(\"Unauthorized\")\n",
    "                case 403:\n",
    "                    dict[\"Response\"].append(\"Forbidden\")\n",
    "                case 404:\n",
    "                    dict[\"Response\"].append(\"Not Found\")\n",
    "                case 408:\n",
    "                    dict[\"Response\"].append(\"Request Timeout\")\n",
    "                case 500:\n",
    "                    dict[\"Response\"].append(\"Internal Server Error\")\n",
    "                case 502:\n",
    "                    dict[\"Response\"].append(\"Bad Gateway\")\n",
    "                case 503:\n",
    "                    dict[\"Response\"].append(\"Service Unavilable\")\n",
    "                case 504:\n",
    "                    dict[\"Response\"].append(\"Gateway Timeout\")\n",
    "                case _:\n",
    "                    dict[\"Response\"].append(\"Undefined\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Display and export the link status table to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Base URL",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Link URL",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Link Status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Status Code",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Response",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0cb06ddf-ef57-414e-b2e1-cf705412f664",
       "rows": [
        [
         "0",
         "https://www.example.com/",
         "https://www.iana.org/domains/example",
         "Active",
         "200",
         "Success"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base URL</th>\n",
       "      <th>Link URL</th>\n",
       "      <th>Link Status</th>\n",
       "      <th>Status Code</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.example.com/</td>\n",
       "      <td>https://www.iana.org/domains/example</td>\n",
       "      <td>Active</td>\n",
       "      <td>200</td>\n",
       "      <td>Success</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Base URL                              Link URL Link Status  \\\n",
       "0  https://www.example.com/  https://www.iana.org/domains/example      Active   \n",
       "\n",
       "   Status Code Response  \n",
       "0          200  Success  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% create dataframe and display results\n",
    "\n",
    "links_status_df = pd.DataFrame(dict)\n",
    "display(links_status_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% export links table\n",
    "\n",
    "current_date = dt.today().strftime(\"%m-%d-%y\")\n",
    "links_status_df.to_csv(os.path.join(os.getcwd(), f\"LinksChecker_Output_{current_date}.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bsesc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
